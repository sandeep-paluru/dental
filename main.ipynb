{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b19f65f9-fb12-4bfb-a710-85a84873977d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyaudio in d:\\anaconda\\lib\\site-packages (0.2.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f18c01be-3dcc-44bf-baad-51975150e39a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vosk in d:\\anaconda\\lib\\site-packages (0.3.45)\n",
      "Requirement already satisfied: cffi>=1.0 in d:\\anaconda\\lib\\site-packages (from vosk) (1.17.1)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (from vosk) (2.32.3)\n",
      "Requirement already satisfied: srt in d:\\anaconda\\lib\\site-packages (from vosk) (3.5.3)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\lib\\site-packages (from vosk) (4.66.5)\n",
      "Requirement already satisfied: websockets in d:\\anaconda\\lib\\site-packages (from vosk) (15.0.1)\n",
      "Requirement already satisfied: pycparser in d:\\anaconda\\lib\\site-packages (from cffi>=1.0->vosk) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests->vosk) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests->vosk) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests->vosk) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests->vosk) (2025.1.31)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm->vosk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install vosk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22a89f0b-cada-4ad6-b5af-79ffc37100bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Using cached llama_cpp_python-0.3.8.tar.gz (67.3 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in d:\\anaconda\\lib\\site-packages (from llama-cpp-python) (4.11.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in d:\\anaconda\\lib\\site-packages (from llama-cpp-python) (1.26.4)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in d:\\anaconda\\lib\\site-packages (from llama-cpp-python) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.3)\n",
      "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'error'\n",
      "Failed to build llama-cpp-python\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for llama-cpp-python (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [20 lines of output]\n",
      "  \u001b[32m*** \u001b[1mscikit-build-core 0.11.1\u001b[0m using \u001b[34mCMake 4.0.0\u001b[39m\u001b[0m \u001b[31m(wheel)\u001b[0m\n",
      "  \u001b[32m***\u001b[0m \u001b[1mConfiguring CMake...\u001b[0m\n",
      "  2025-04-07 23:21:00,283 - scikit_build_core - WARNING - Can't find a Python library, got libdir=None, ldlibrary=None, multiarch=None, masd=None\n",
      "  loading initial cache file C:\\Users\\palur\\AppData\\Local\\Temp\\tmpb06q_d7m\\build\\CMakeInit.txt\n",
      "  -- Building for: NMake Makefiles\n",
      "  CMake Error at CMakeLists.txt:3 (project):\n",
      "    Running\n",
      "  \n",
      "     'nmake' '-?'\n",
      "  \n",
      "    failed with:\n",
      "  \n",
      "     no such file or directory\n",
      "  \n",
      "  \n",
      "  CMake Error: CMAKE_C_COMPILER not set, after EnableLanguage\n",
      "  CMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage\n",
      "  -- Configuring incomplete, errors occurred!\n",
      "  \u001b[31m\n",
      "  \u001b[1m***\u001b[0m \u001b[31mCMake configuration failed\u001b[0m\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for llama-cpp-python\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (llama-cpp-python)\n"
     ]
    }
   ],
   "source": [
    "pip install llama-cpp-python --prefer-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2891b23d-e048-4059-ad4d-4149c1ef0382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cmakeNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached cmake-4.0.0-py3-none-win_amd64.whl.metadata (6.3 kB)\n",
      "Using cached cmake-4.0.0-py3-none-win_amd64.whl (36.7 MB)\n",
      "Installing collected packages: cmake\n",
      "Successfully installed cmake-4.0.0\n"
     ]
    }
   ],
   "source": [
    "pip install cmake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57fa8833-1898-4ca5-83a7-1b1cafa12eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Using cached llama_cpp_python-0.3.8.tar.gz (67.3 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in d:\\anaconda\\lib\\site-packages (from llama-cpp-python) (4.11.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in d:\\anaconda\\lib\\site-packages (from llama-cpp-python) (1.26.4)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in d:\\anaconda\\lib\\site-packages (from llama-cpp-python) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.3)\n",
      "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'error'\n",
      "Failed to build llama-cpp-python\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for llama-cpp-python (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [20 lines of output]\n",
      "  \u001b[32m*** \u001b[1mscikit-build-core 0.11.1\u001b[0m using \u001b[34mCMake 4.0.0\u001b[39m\u001b[0m \u001b[31m(wheel)\u001b[0m\n",
      "  \u001b[32m***\u001b[0m \u001b[1mConfiguring CMake...\u001b[0m\n",
      "  2025-04-07 23:21:35,358 - scikit_build_core - WARNING - Can't find a Python library, got libdir=None, ldlibrary=None, multiarch=None, masd=None\n",
      "  loading initial cache file C:\\Users\\palur\\AppData\\Local\\Temp\\tmphw3ekaku\\build\\CMakeInit.txt\n",
      "  -- Building for: NMake Makefiles\n",
      "  CMake Error at CMakeLists.txt:3 (project):\n",
      "    Running\n",
      "  \n",
      "     'nmake' '-?'\n",
      "  \n",
      "    failed with:\n",
      "  \n",
      "     no such file or directory\n",
      "  \n",
      "  \n",
      "  CMake Error: CMAKE_C_COMPILER not set, after EnableLanguage\n",
      "  CMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage\n",
      "  -- Configuring incomplete, errors occurred!\n",
      "  \u001b[31m\n",
      "  \u001b[1m***\u001b[0m \u001b[31mCMake configuration failed\u001b[0m\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for llama-cpp-python\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (llama-cpp-python)\n"
     ]
    }
   ],
   "source": [
    "pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1dfb6cce-8da2-487a-bb4f-7fefddbec072",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n",
      "Recognized text: oh my god\n",
      "Recognized text: i was simply won't sell out there these make very claims that make repay say that is very mysterious default my now with those are about later on\n",
      "Recognized text: now but split alice start seeing and experiencing a haunted me and stop again at that wage job get like she goes out into this desert that the housewives aren't allowed to go into and she touches a building and wakes up back home a very strange is this we're plane crash and a window closes in on earth is what's going on with all that don't worry about it in this other housewife\n",
      "Recognized text: being forced to take medication cause she keeps saying nothing is really what is the wouldn't what is the medication do the worry about that either i wanna know of a little worried about well stop at a than alice randomly wraps her own head in plastic wrap and that other housewife seemingly dies but not really okay yeah you know sir that's a lot of weird stuff and so i guess alice is gonna start out piece the\n",
      "Recognized text: gather with this is all about as the weird things happen that really know just kind of a string of weird occurrences for about ninety minutes or so with little to no explanation or information revealed out gao kind of and suddenly a bunch of information's going to drop of like a third of the movie last okay great so what do we find ourselves or a turns out the whole town is a me like\n",
      "Recognized text: and should have and like the matrix\n",
      "Recognized text: \n",
      "Recognized text: \n",
      "Recognized text: i doubt that alice's husband jack in the real world is unemployed and he'd been kind are radicalized my that guy franco he has todd cat and internet forums he'd been made to hate being a luger while is why approves of money and he wants that traditional live of you know the waves days and almond and groups and\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 37\u001b[0m         data \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m4000\u001b[39m)  \u001b[38;5;66;03m# Read the audio data in chunks of 4000 bytes\u001b[39;00m\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m recognizer\u001b[38;5;241m.\u001b[39mAcceptWaveform(data):\n\u001b[0;32m     39\u001b[0m             result \u001b[38;5;241m=\u001b[39m recognizer\u001b[38;5;241m.\u001b[39mResult()  \u001b[38;5;66;03m# Get the recognized result (returns a JSON string)\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\pyaudio\\__init__.py:570\u001b[0m, in \u001b[0;36mPyAudio.Stream.read\u001b[1;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_input:\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot input stream\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    569\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[1;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mread_stream(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream, num_frames,\n\u001b[0;32m    571\u001b[0m                       exception_on_overflow)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyaudio\n",
    "from vosk import Model, KaldiRecognizer\n",
    "import json  # Import the JSON module to parse the result\n",
    "\n",
    "# Set the path to the model you downloaded\n",
    "MODEL_PATH = \"model\"  # Replace with the actual model path\n",
    "\n",
    "# Try loading the Vosk model\n",
    "try:\n",
    "    model = Model(MODEL_PATH)\n",
    "    recognizer = KaldiRecognizer(model, 16000)  # 16000 is the sample rate\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    exit(1)  # Exit the script if the model cannot be loaded\n",
    "\n",
    "# Initialize PyAudio for microphone input\n",
    "try:\n",
    "    p = pyaudio.PyAudio()\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing microphone: {e}\")\n",
    "    exit(1)  # Exit if the microphone cannot be initialized\n",
    "\n",
    "# Try opening the microphone stream\n",
    "try:\n",
    "    stream = p.open(format=pyaudio.paInt16, channels=1, rate=16000, input=True, frames_per_buffer=4000)\n",
    "    stream.start_stream()\n",
    "except Exception as e:\n",
    "    print(f\"Error opening microphone stream: {e}\")\n",
    "    exit(1)  # Exit if the microphone stream cannot be opened\n",
    "\n",
    "print(\"Listening...\")\n",
    "\n",
    "# Continuously listen to the microphone and process audio\n",
    "while True:\n",
    "    try:\n",
    "        data = stream.read(4000)  # Read the audio data in chunks of 4000 bytes\n",
    "        if recognizer.AcceptWaveform(data):\n",
    "            result = recognizer.Result()  # Get the recognized result (returns a JSON string)\n",
    "\n",
    "            # Parse the JSON result to get the recognized text\n",
    "            result_dict = json.loads(result)\n",
    "\n",
    "            # Print the recognized text\n",
    "            print(f\"Recognized text: {result_dict['text']}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during speech recognition: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b9e061d-fbc5-4854-9056-007b54b90d49",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_cpp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama\n\u001b[0;32m      3\u001b[0m llm \u001b[38;5;241m=\u001b[39m Llama(model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/models/phi-2.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'llama_cpp'"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(model_path=\"D:/models/phi-2.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f5a227-9ca8-4b89-a9ea-24ab5a2348a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"You are a helpful dental clinic assistant. If someone asks about appointments, guide them. If they ask about parking, explain location.\"\n",
    "\n",
    "while True:\n",
    "    question = input(\"You: \")\n",
    "    full_prompt = prompt + f\"\\nUser: {question}\\nAssistant:\"\n",
    "    output = llm(full_prompt, max_tokens=150, stop=[\"User:\"], echo=False)\n",
    "    print(\"Assistant:\", output['choices'][0]['text'].strip())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
